{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp conll_reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A NER tagger using AllenNLP \n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    }
   ],
   "source": [
    "#export\n",
    "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
    "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
    "from allennlp.data.instance import Instance\n",
    "from allennlp.data.tokenizers import Token\n",
    "from allennlp.data.fields import Field, TextField, SequenceLabelField\n",
    "from overrides import overrides\n",
    "from tqdm import tqdm\n",
    "from typing import List, Dict, Iterator\n",
    "from itertools import groupby\n",
    "path_data = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@DatasetReader.register(\"conll_03_reader\")\n",
    "class CoNLL03DatasetReader(DatasetReader): \n",
    "    def __init__(self,\n",
    "                 token_indexers:Dict[str, TokenIndexer] = None,\n",
    "                 lazy:bool = True) -> None: \n",
    "        \"\"\"\n",
    "        token_indexers are like tok2idx, basically. \n",
    "        lazy requred for anything inheriting from DatasetReader, and it is used\n",
    "        to read only batches of data at once instead of putting entire dataset in memory, \n",
    "        so it is useful for large data. \n",
    "        \"\"\"\n",
    "        super().__init__(lazy)\n",
    "        self._token_indexers = token_indexers or {'tokens': SingleIdTokenIndexer()}\n",
    "        \n",
    "    @overrides\n",
    "    def _read(self, file_path: str) -> Iterator[Instance]:\n",
    "        # function to detect empty lines\n",
    "        is_divider = lambda line: line.strip() == ''\n",
    "        # open file, go through each line (read as string)\n",
    "        with open(file_path, 'r') as conll_file:\n",
    "            # group each sentence together in lists\n",
    "            # each sentence is \"sandwiched\" by blank lines either side\n",
    "            for divider, lines in itertools.groupby(conll_file, is_divider):\n",
    "                if not divider:  # ignore blank lines\n",
    "                    # string -> list\n",
    "                    fields = [l.strip().split() for l in lines]\n",
    "                    # switch it so that each field is a list of tokens/labels\n",
    "                    # kind of like a transpose function\n",
    "                    fields = [l for l in zip(*fields)]\n",
    "                    # only keep the tokens and NER labels\n",
    "                    tokens, _, _, ner_tags = fields\n",
    "                    # convert to instance\n",
    "                    yield self.text_to_instance(tokens, ner_tags)\n",
    "        \n",
    "    @overrides\n",
    "    def text_to_instance(self, words: List[str], ner_tags: List[str]) -> Instance:\n",
    "        fields: Dict[str, Field] = {}\n",
    "        tokens = TextField([Token(o) for o in words],self._token_indexers )\n",
    "        fields['tokens'] = tokens\n",
    "        fields['label'] = SequenceLabelField(ner_tags, tokens)\n",
    "        return Instance(fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
