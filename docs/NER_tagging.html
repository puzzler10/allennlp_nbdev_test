---

title: A NER tagger using AllenNLP 

keywords: fastai
sidebar: home_sidebar

summary: "API details."
description: "API details."
---
<!--

#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: NER_tagging.ipynb
# command to build the docs after a change: nbdev_build_docs

-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>wandb: WARNING W&amp;B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>wandb: WARNING W&amp;B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_markdown rendered_html output_subarea ">
<h2 id="CoNLL03DatasetReader" class="doc_header"><code>class</code> <code>CoNLL03DatasetReader</code><a href="https://github.com/Tom Roth/tagger/tree/master/tagger/conll_reader.py#L19" class="source_link" style="float:right">[source]</a></h2><blockquote><p><code>CoNLL03DatasetReader</code>(<strong><code>token_indexers</code></strong>:<code>Dict</code>[<code>str</code>, <code>TokenIndexer</code>]=<em><code>None</code></em>, <strong><code>lazy</code></strong>:<code>bool</code>=<em><code>True</code></em>) :: <code>DatasetReader</code></p>
</blockquote>
<p>A <code>DatasetReader</code> knows how to turn a file containing a dataset into a collection
of <code>Instances</code>.  To implement your own, just override the <code>_read(file_path)</code> method
to return an <code>Iterable</code> of the instances. This could be a list containing the instances
or a lazy generator that returns them one at a time.</p>
<p>All parameters necessary to <code>_read</code> the data apart from the filepath should be passed
to the constructor of the <code>DatasetReader</code>.</p>
<h1 id="Parameters">Parameters<a class="anchor-link" href="#Parameters"> </a></h1><p>lazy : <code>bool</code>, optional (default=<code>False</code>)
    If this is true, <code>instances()</code> will return an object whose <code>__iter__</code> method
    reloads the dataset each time it's called. Otherwise, <code>instances()</code> returns a list.</p>
<p>cache_directory : <code>str</code>, optional (default=<code>None</code>)
    If given, we will use this directory to store a cache of already-processed <code>Instances</code> in
    every file passed to :func:<code>read</code>, serialized (by default, though you can override this) as
    one string-formatted <code>Instance</code> per line.  If the cache file for a given <code>file_path</code> exists,
    we read the <code>Instances</code> from the cache instead of re-processing the data (using
    :func:<code>_instances_from_cache_file</code>).  If the cache file does <em>not</em> exist, we will <em>create</em>
    it on our first pass through the data (using :func:<code>_instances_to_cache_file</code>).</p>

<pre><code>!!! NOTE
    It is the _caller's_ responsibility to make sure that this directory is
    unique for any combination of code and parameters that you use.  That is, if you pass a
    directory here, we will use any existing cache files in that directory _regardless of the
    parameters you set for this DatasetReader!_

</code></pre>
<p>max_instances : <code>int</code>, optional (default=<code>None</code>)
    If given, will stop reading after this many instances. This is a useful setting for debugging.
    Setting this disables caching.</p>
<p>manual_distributed_sharding: <code>bool</code>, optional (default=<code>False</code>)
    By default, when used in a distributed setting, <code>DatasetReader</code> makes sure that each
    worker process only receives a subset of the data. It does this by reading the whole
    dataset in each worker, but filtering out the instances that are not needed. If you
    can implement a faster mechanism that only reads part of the data, set this to True,
    and do the sharding yourself.</p>
<p>manual_multi_process_sharding : <code>bool</code>, optional (default=<code>False</code>)
    This is similar to the <code>manual_distributed_sharding</code> parameter, but applies to
    multi-process data loading. By default, when this reader is used by a multi-process
    data loader (i.e. a <code>DataLoader</code> with <code>num_workers &gt; 1</code>), each worker will
    filter out all but a subset of the instances that are needed so that you
    don't end up with duplicates.</p>

<pre><code>!!! NOTE
    **There is really no benefit of using a multi-process
    `DataLoader` unless you can specifically implement a faster sharding mechanism
    within `_read()`**. In that case you should set `manual_multi_process_sharding`
    to `True`.</code></pre>

</div>

</div>

</div>
</div>

</div>
    {% endraw %}

</div>
 

